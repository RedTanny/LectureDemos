{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain huggingface_hub transformers accelerate langchain-groq langchain_community\n",
    "!pip install langchain-huggingface langchain_openai\n",
    "!pip install python-dotenv\n",
    "\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation took around 5min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load  configuration from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "os.environ[\"GROQ_API_KEY\"] = config[\"GROQ_API_KEY\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = config[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#print(config)\n",
    "#print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **A few words on LLM models**:\n",
    "\n",
    " -Mistral-7B-v0.1: This is a base language model from Mistral AI with 7 billion parameters, designed for general language understanding tasks without any specialized fine-tuning.\n",
    "\n",
    "-Mistral-7B-Instruct-v0.2: An instruction-tuned variant of Mistral-7B, fine-tuned on a range of instruction-following datasets to better perform on tasks where following explicit instructions is required.\n",
    "\n",
    "-Mixtral-8x7b-32768: A composite model made up of eight Mistral-7B models, optimized for parallel processing on hardware like the GroqChip, suitable for large-scale inference with a focus on performance and efficiency.\n",
    "\n",
    "-LLaMA3-8B-8192: A third-generation LLaMA model with 8 billion parameters, designed for more efficient context window management and capable of handling longer sequences of up to 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_base_model_id     = \"mistralai/Mistral-7B-v0.1\"\n",
    "hg_instruct_model_id = \"mistralai/Mistral-7B-Instruct-v0.3\" #\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "groq_model_mixtral   = \"mixtral-8x7b-32768\"\n",
    "groq_model_llama3    = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîÅ **Workflow Overview**:\n",
    "   - Select model type (LLM or Chat) for your task.\n",
    "   - Design a prompt to guide the model.\n",
    "   - Run input through your chosen model.\n",
    "   - Use Output Parser for neat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "   \n",
    "llm = HuggingFaceEndpoint(repo_id=hg_instruct_model_id)\n",
    "#Logic question\n",
    "_question= \"Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\"\n",
    "print(llm.invoke(_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mistral based model\")\n",
    "print(\"-------------------\")\n",
    "llm = HuggingFaceEndpoint(repo_id=hg_base_model_id)\n",
    "print(llm.invoke(_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at grok responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage,SystemMessage\n",
    "\n",
    "llm_groq = ChatGroq(model=groq_model_llama3)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"When I ask for help to write something, you will reply with a document that contains at least one joke\"),\n",
    "    HumanMessage(content=\"write a cpp hello world.\")\n",
    "]\n",
    "\n",
    "response = llm_groq.invoke(messages)\n",
    "print(type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "\n",
    "- Output parsers shape the AI's text output into a more usable form, like a database entry or a JSON object.\n",
    "\n",
    "**Main Uses:**\n",
    "\n",
    "1. They turn a block of text into organized data.\n",
    "2. They can guide the AI on how to format its responses for consistency and ease of use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.list import ListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}\",\n",
    "    input_variables=[\"things\"])\n",
    "\n",
    "response = llm_groq.invoke(input=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use the parsers instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}.\\n {format_instructions} reply only results\",\n",
    "    input_variables=[\"things\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "output = llm_groq.predict(text=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(type(output))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can parse the output to a list (Python object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL) Overview**:\n",
    "   - **Purpose**: Simplify building complex chains from basic components.\n",
    "   - **Features**: Supports streaming, parallelism, and logging.\n",
    "\n",
    "### üõ†Ô∏è Basic Use Case: Prompt + Model + Output Parser\n",
    "   - **Common Approach**: Link a prompt template with a model.\n",
    "   - **Chain Mechanism**: Using the `|` symbol, like a Unix pipe, to connect components.\n",
    "   - **Process Flow**: User input ‚Üí Prompt Template ‚Üí Model ‚Üí Output Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "\n",
    "model = ChatGroq(model=groq_model_llama3,temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
