{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain huggingface_hub transformers accelerate langchain-groq langchain_community\n",
    "!pip install langchain-huggingface langchain_openai\n",
    "!pip install python-dotenv\n",
    "\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation took around 5min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load  configuration from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "os.environ[\"GROQ_API_KEY\"] = config[\"GROQ_API_KEY\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = config[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#print(config)\n",
    "#print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **A few words on LLM models**:\n",
    "\n",
    " -Mistral-7B-v0.1: This is a base language model from Mistral AI with 7 billion parameters, designed for general language understanding tasks without any specialized fine-tuning.\n",
    "\n",
    "-Mistral-7B-Instruct-v0.2: An instruction-tuned variant of Mistral-7B, fine-tuned on a range of instruction-following datasets to better perform on tasks where following explicit instructions is required.\n",
    "\n",
    "-Mixtral-8x7b-32768: A composite model made up of eight Mistral-7B models, optimized for parallel processing on hardware like the GroqChip, suitable for large-scale inference with a focus on performance and efficiency.\n",
    "\n",
    "-LLaMA3-8B-8192: A third-generation LLaMA model with 8 billion parameters, designed for more efficient context window management and capable of handling longer sequences of up to 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_base_model_id     = \"mistralai/Mistral-7B-v0.1\"\n",
    "hg_instruct_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "groq_model_mixtral   = \"mixtral-8x7b-32768\"\n",
    "groq_model_llama3    = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-do_sample -  parameter determines whether the model should use sampling for text generation.\n",
    " When do_sample is set to True, the model generates text using sampling methods like temperature sampling, top-k sampling, or top-p (nucleus) sampling. This introduces randomness into the output, making it more diverse and creative. This is often used in creative writing task\n",
    " When do_sample is set to False, the model generates text deterministically by always choosing the token with the highest probability (i.e., greedy decoding). This results in more predictable and consistent outputs but can be less creative.\n",
    "\n",
    " -The early_stopping parameter controls whether the generation process should stop when the end-of-sequence token (EOS) is generated.\n",
    "\n",
    "-The temperature parameter controls the randomness\n",
    "  Lower values (e.g., temperature=0.1): The model becomes more conservative, meaning it is more likely to select the highest probability \n",
    "  Higher values (e.g., temperature=1.0 or temperature=1.5): The model becomes more random and exploratory in its outputs. making the text more diverse and creative. However, very high temperatures can lead to outputs that are less coherent and more erratic.\n",
    "\n",
    "-The max_length parameter defines the maximum number of tokens the model is allowed to generate in a single output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_params(temperature,max_length,do_sample,early_stopping) -> dict:\n",
    "        model_kwargs={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_length\": max_length,\n",
    "            \"do_sample\":do_sample,\n",
    "            \"early_stopping\":early_stopping\n",
    "            }\n",
    "        return model_kwargs\n",
    "    \n",
    "math_params_llm    = make_model_params(0.1,125,False,True)\n",
    "\n",
    "general_params_llm = make_model_params(0.7,125,False,True)\n",
    "\n",
    "story_params_llm   = make_model_params(1.0,400,True,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math LLM Params**\n",
    "\n",
    " - By setting do_sample=False, temperature to a low value, early_stopping=True, and choosing an appropriate max_length, you optimize \n",
    " the model for generating precise and accurate answers to math or logic questions.\n",
    " \n",
    "**General LLM Params**\n",
    " - do_sample=False ensures more deterministic outputs that stick to the most likely responses.\n",
    " - A moderate temperature (around 0.7) balances creativity with coherence, making the output informative but not too random or repetitive.\n",
    " - early_stopping: True\n",
    "  Why: Ensures that the response ends at a logical conclusion rather than generating excessive text.\n",
    "\n",
    "**StoryLine LLM params**\n",
    " - do_sample Enables the model to use sampling techniques to create diverse and creative outputs, which is essential for storytelling.\n",
    " - temperature 1.0  A higher temperature encourages more creativity and variability in the text, making stories more engaging and less predictable.\n",
    " - early_stopping: False . Allows the story to develop more fully without being prematurely cut off by an end-of-sequence token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "🔁 **Workflow Overview**:\n",
    "   - Select model type (LLM or Chat) for your task.\n",
    "   - Design a prompt to guide the model.\n",
    "   - Run input through your chosen model.\n",
    "   - Use Output Parser for neat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def run_hg_llm_chain(repo_id: str,\n",
    "                  question: str,\n",
    "                  model_prams) -> str:\n",
    "    \"\"\"\n",
    "    Initializes a HuggingFaceHub with specified parameters and runs an LLMChain with a given question.\n",
    "\n",
    "    Args:\n",
    "    repo_id (str): The repository ID for the HuggingFace model.\n",
    "    question (str): The question to be passed to the LLMChain.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    str: The result from running the LLMChain.\n",
    "    \"\"\"\n",
    "    # Define the prompt \n",
    "    \n",
    "    #llm = HuggingFaceHub(\n",
    "    #    repo_id=repo_id,\n",
    "    #    model_kwargs=model_kwargs\n",
    "    #)\n",
    "    model_kwargs2={            \n",
    "            \"early_stopping\":model_prams[\"early_stopping\"]\n",
    "            }\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=repo_id,\n",
    "        max_new_tokens = model_prams[\"max_length\"],        \n",
    "        temperature= model_prams[\"temperature\"],\n",
    "        do_sample= model_prams[\"do_sample\"],\n",
    "        model_kwargs=model_kwargs2               \n",
    "    )\n",
    "    return llm.invoke(question)\n",
    "\n",
    "#Logic question\n",
    "_question= \"Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\"\n",
    "print(\"mistral intruct model\")\n",
    "print(\"-------------------\")\n",
    "print(run_hg_llm_chain(hg_instruct_model_id,_question,math_params_llm))\n",
    "print(\"-- change llm params\")\n",
    "print(run_hg_llm_chain(hg_instruct_model_id,_question,story_params_llm))\n",
    "print(\"-------------------\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mistral based model\")\n",
    "print(\"-------------------\")\n",
    "print(run_hg_llm_chain(hg_base_model_id,_question,math_params_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at grok responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage,SystemMessage\n",
    "\n",
    "llm_groq = ChatGroq(model=groq_model_llama3)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"When I ask for help to write something, you will reply with a document that contains at least one joke\"),\n",
    "    HumanMessage(content=\"write a cpp hello world.\")\n",
    "]\n",
    "\n",
    "response = llm_groq.invoke(messages)\n",
    "print(type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "\n",
    "- Output parsers shape the AI's text output into a more usable form, like a database entry or a JSON object.\n",
    "\n",
    "**Main Uses:**\n",
    "\n",
    "1. They turn a block of text into organized data.\n",
    "2. They can guide the AI on how to format its responses for consistency and ease of use.\n",
    "\n",
    "This stripped-down explanation keeps the focus on the purpose and function of output parsers, suitable for a quick overview during a presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.list import ListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}\",\n",
    "    input_variables=[\"things\"])\n",
    "\n",
    "response = llm_groq.invoke(input=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use the parsers instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}.\\n {format_instructions} reply only results\",\n",
    "    input_variables=[\"things\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "output = llm_groq.predict(text=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(type(output))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can parse the output to a list (Python object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL) Overview**:\n",
    "   - **Purpose**: Simplify building complex chains from basic components.\n",
    "   - **Features**: Supports streaming, parallelism, and logging.\n",
    "\n",
    "### 🛠️ Basic Use Case: Prompt + Model + Output Parser\n",
    "   - **Common Approach**: Link a prompt template with a model.\n",
    "   - **Chain Mechanism**: Using the `|` symbol, like a Unix pipe, to connect components.\n",
    "   - **Process Flow**: User input → Prompt Template → Model → Output Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "\n",
    "model = ChatGroq(model=groq_model_llama3,temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
