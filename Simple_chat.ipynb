{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain cohere huggingface_hub transformers accelerate langchain-groq\n",
    "!pip install python-dotenv\n",
    "\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load  configuration from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'USER': 'nokia', 'VSCODE_WSL_EXT_LOCATION': '/mnt/c/Users/tanny/.vscode/extensions/ms-vscode-remote.remote-wsl-0.88.2', 'SHLVL': '1', 'WT_PROFILE_ID': '{2c4de342-38b7-51cf-b940-2309a097f518}', 'HOME': '/home/nokia', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'WSL_DISTRO_NAME': 'Ubuntu', 'WAYLAND_DISPLAY': 'wayland-0', 'LOGNAME': 'nokia', 'NAME': 'Code', 'WSL_INTEROP': '/run/WSL/479250_interop', 'PULSE_SERVER': 'unix:/mnt/wslg/PulseServer', '_': '/home/nokia/LangChainPromptCourse/venv/bin/python', 'TERM': 'xterm-color', 'PATH': '/home/nokia/LangChainPromptCourse/venv/bin:/home/nokia/.vscode-server/bin/fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/bin/remote-cli:/home/nokia/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Windows/CCM/:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Windows/system32/config/systemprofile/AppData/Local/Microsoft/WindowsApps:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X64/:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X86/:/mnt/c/Program Files (x86)/Common Files/Pulse Secure/TNC Client Plugin/:/mnt/c/Program Files/PuTTY/:/mnt/c/Users/tanny/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/tanny/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/tanny/AppData/Local/GitHubDesktop/bin:/snap/bin:/usr/local/go/bin', 'WT_SESSION': '2b925101-6f7a-48c3-8a85-9bbfeb7b4b77', 'XDG_RUNTIME_DIR': '/run/user/1000/', 'DISPLAY': ':0', 'LANG': 'C.UTF-8', 'SHELL': '/bin/bash', 'PWD': '/mnt/c/Users/tanny/AppData/Local/Programs/Microsoft VS Code', 'WSL2_GUI_APPS_ENABLED': '1', 'HOSTTYPE': 'x86_64', 'WSLENV': 'ELECTRON_RUN_AS_NODE/w:WT_SESSION:WT_PROFILE_ID:\\n', 'VSCODE_CWD': '/mnt/c/Users/tanny/AppData/Local/Programs/Microsoft VS Code', 'VSCODE_NLS_CONFIG': '{\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/nokia/.vscode-server/bin/fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}', 'VSCODE_HANDLES_SIGPIPE': 'true', 'MOTD_SHOWN': 'update-motd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'LIBVIRT_DEFAULT_URI': 'qemu:///system', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'ELECTRON_RUN_AS_NODE': '1', 'VSCODE_IPC_HOOK_CLI': '/run/user/1000/vscode-ipc-7f55993d-8db1-4d7b-97cb-f5ffb3865e9d.sock', 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': '1', 'PYTHONUNBUFFERED': '1', 'OPENAI_API_KEY': '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', 'VIRTUAL_ENV': '/home/nokia/LangChainPromptCourse/venv', 'HUGGINGFACEHUB_API_TOKEN': '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', 'PYTHONIOENCODING': 'utf-8', 'GROQ_API_KEY': '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', 'VIRTUAL_ENV_PROMPT': '(venv) ', 'PS1': '(venv) ', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'PYTHON_FROZEN_MODULES': 'on', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")\n",
    "os.environ[\"GROQ_API_KEY\"] = config[\"GROQ_API_KEY\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = config[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "#print(config)\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **A few words on LLM models**:\n",
    "\n",
    " -Mistral-7B-v0.1: This is a base language model from Mistral AI with 7 billion parameters, designed for general language understanding tasks without any specialized fine-tuning.\n",
    "\n",
    "-Mistral-7B-Instruct-v0.2: An instruction-tuned variant of Mistral-7B, fine-tuned on a range of instruction-following datasets to better perform on tasks where following explicit instructions is required.\n",
    "\n",
    "-Mixtral-8x7b-32768: A composite model made up of eight Mistral-7B models, optimized for parallel processing on hardware like the GroqChip, suitable for large-scale inference with a focus on performance and efficiency.\n",
    "\n",
    "-LLaMA3-8B-8192: A third-generation LLaMA model with 8 billion parameters, designed for more efficient context window management and capable of handling longer sequences of up to 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_base_model_id     = \"mistralai/Mistral-7B-v0.1\"\n",
    "hg_instruct_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "groq_model_mixtral   = \"mixtral-8x7b-32768\"\n",
    "groq_model_llama3    = \"llama3-8b-8192\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-do_sample -  parameter determines whether the model should use sampling for text generation.\n",
    " When do_sample is set to True, the model generates text using sampling methods like temperature sampling, top-k sampling, or top-p (nucleus) sampling. This introduces randomness into the output, making it more diverse and creative. This is often used in creative writing task\n",
    " When do_sample is set to False, the model generates text deterministically by always choosing the token with the highest probability (i.e., greedy decoding). This results in more predictable and consistent outputs but can be less creative.\n",
    "\n",
    " -The early_stopping parameter controls whether the generation process should stop when the end-of-sequence token (EOS) is generated.\n",
    "\n",
    "-The temperature parameter controls the randomness\n",
    "  Lower values (e.g., temperature=0.1): The model becomes more conservative, meaning it is more likely to select the highest probability \n",
    "  Higher values (e.g., temperature=1.0 or temperature=1.5): The model becomes more random and exploratory in its outputs. making the text more diverse and creative. However, very high temperatures can lead to outputs that are less coherent and more erratic.\n",
    "\n",
    "-The max_length parameter defines the maximum number of tokens the model is allowed to generate in a single output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_params(temperature,max_length,do_sample,early_stopping) -> dict:\n",
    "        model_kwargs={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_length\": max_length,\n",
    "            \"do_sample\":do_sample,\n",
    "            \"early_stopping\":early_stopping\n",
    "            }\n",
    "        return model_kwargs\n",
    "    \n",
    "math_params_llm    = make_model_params(0.2,50,False,True)\n",
    "\n",
    "general_params_llm = make_model_params(0.7,100,False,True)\n",
    "\n",
    "story_params_llm   = make_model_params(1.0,400,True,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Math LLM Params**\n",
    "\n",
    " - By setting do_sample=False, temperature to a low value, early_stopping=True, and choosing an appropriate max_length, you optimize \n",
    " the model for generating precise and accurate answers to math or logic questions.\n",
    " \n",
    "**General LLM Params**\n",
    " - do_sample=False ensures more deterministic outputs that stick to the most likely responses.\n",
    " - A moderate temperature (around 0.7) balances creativity with coherence, making the output informative but not too random or repetitive.\n",
    " - early_stopping: True\n",
    "  Why: Ensures that the response ends at a logical conclusion rather than generating excessive text.\n",
    "\n",
    "**StoryLine LLM params**\n",
    " - do_sample Enables the model to use sampling techniques to create diverse and creative outputs, which is essential for storytelling.\n",
    " - temperature 1.0  A higher temperature encourages more creativity and variability in the text, making stories more engaging and less predictable.\n",
    " - early_stopping: False . Allows the story to develop more fully without being prematurely cut off by an end-of-sequence token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "üîÅ **Workflow Overview**:\n",
    "   - Select model type (LLM or Chat) for your task.\n",
    "   - Design a prompt to guide the model.\n",
    "   - Run input through your chosen model.\n",
    "   - Use Output Parser for neat results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral intruct model\n",
      "-------------------\n",
      "Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\n",
      "Answer: In the bedroom, assuming Bob did not spill the ball while carrying it or while turning the cup upside down in the bedroom.\n",
      "-- change llm params\n",
      "Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\n",
      "Answer: In the bedroom, still in the cup when Bob left it.\n",
      "\n",
      "The description says Bob carried the cup to the bedroom, turned it upside down (implying that's where he emptied it), and then went to the garden. There's no mention of him doing anything with the ball in the garden or on the way there. So the ball must have stayed in the cup when Bob carried it to the bedroom.\n",
      "-------------------\n",
      "mistral based model\n",
      "-------------------\n",
      "Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\n",
      "\n",
      "Answer:\n",
      "\n",
      "The ball is in the cup.\n",
      "\n",
      "Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\n",
      "\n",
      "Answer:\n",
      "\n",
      "The ball is in the cup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "def run_hg_llm_chain(repo_id: str,\n",
    "                  question: str,\n",
    "                  model_kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Initializes a HuggingFaceHub with specified parameters and runs an LLMChain with a given question.\n",
    "\n",
    "    Args:\n",
    "    repo_id (str): The repository ID for the HuggingFace model.\n",
    "    question (str): The question to be passed to the LLMChain.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    str: The result from running the LLMChain.\n",
    "    \"\"\"\n",
    "    # Define the prompt \n",
    "    \n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=repo_id,\n",
    "        model_kwargs=model_kwargs\n",
    "    )\n",
    "    \n",
    "    return llm.invoke(question)\n",
    "\n",
    "#Logic question\n",
    "_question= \"Question:Bob is in the living room. He walks to the kitchen, carrying a cup. He puts a ball in the cup and carries the cup to the bedroom. He turns the cup upside down, then walks to the garden. He puts the cup down in the garden, then walks to the garage. Where is the ball?\"\n",
    "print(\"mistral intruct model\")\n",
    "print(\"-------------------\")\n",
    "print(run_hg_llm_chain(hg_instruct_model_id,_question,math_params_llm))\n",
    "print(\"-- change llm params\")\n",
    "print(run_hg_llm_chain(hg_instruct_model_id,_question,story_params_llm))\n",
    "print(\"-------------------\")\n",
    "print(\"mistral based model\")\n",
    "print(\"-------------------\")\n",
    "print(run_hg_llm_chain(hg_base_model_id,_question,math_params_llm))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at grok responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "Here is a C++ \"Hello, World!\" program:\n",
      "```\n",
      "#include <iostream>\n",
      "\n",
      "int main() {\n",
      "    std::cout << \"Hello, World!\" << std::endl;\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "And as requested, here's a joke to brighten up your day:\n",
      "\n",
      "Why did the C++ programmer quit his job?\n",
      "\n",
      "Because he didn't get arrays! (get a raise, haha)\n",
      "\n",
      "Hope you enjoy your \"Hello, World!\" program!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage,SystemMessage\n",
    "\n",
    "llm_groq = ChatGroq(model=groq_model_llama3)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"When I ask for help to write something, you will reply with a document that contains at least one joke\"),\n",
    "    HumanMessage(content=\"write a cpp hello world.\")\n",
    "]\n",
    "\n",
    "response = llm_groq.invoke(messages)\n",
    "print(type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "\n",
    "- Output parsers shape the AI's text output into a more usable form, like a database entry or a JSON object.\n",
    "\n",
    "**Main Uses:**\n",
    "\n",
    "1. They turn a block of text into organized data.\n",
    "2. They can guide the AI on how to format its responses for consistency and ease of use.\n",
    "\n",
    "This stripped-down explanation keeps the focus on the purpose and function of output parsers, suitable for a quick overview during a presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers.list import ListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three sports that don't use balls:\n",
      "\n",
      "1. Wrestling: This combat sport involves grappling and manipulating an opponent's body to gain control or submit them.\n",
      "2. Boxing: A combat sport that involves punching and defending against punches, but no ball is used.\n",
      "3. Judo: A martial art and combat sport that focuses on throwing, grappling, and submission techniques, but does not involve a ball.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}\",\n",
    "    input_variables=[\"things\"])\n",
    "\n",
    "response = llm_groq.invoke(input=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to use the parsers instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nokia/LangChainPromptCourse/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Wrestling, Boxing, Gymnastics\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 {things}.\\n {format_instructions} reply only results\",\n",
    "    input_variables=[\"things\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "output = llm_groq.predict(text=prompt.format(things=\"sports that don't use balls\"))\n",
    "print(type(output))\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can parse the output to a list (Python object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wrestling', 'Boxing', 'Gymnastics']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Expression Language (LCEL) Overview**:\n",
    "   - **Purpose**: Simplify building complex chains from basic components.\n",
    "   - **Features**: Supports streaming, parallelism, and logging.\n",
    "\n",
    "### üõ†Ô∏è Basic Use Case: Prompt + Model + Output Parser\n",
    "   - **Common Approach**: Link a prompt template with a model.\n",
    "   - **Chain Mechanism**: Using the `|` symbol, like a Unix pipe, to connect components.\n",
    "   - **Process Flow**: User input ‚Üí Prompt Template ‚Üí Model ‚Üí Output Parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "\n",
    "model = ChatGroq(model=groq_model_llama3,temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Cookies and Cream',\n",
       " 'Mint Chocolate Chip']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
