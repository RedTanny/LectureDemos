{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":25092,"status":"ok","timestamp":1706242390622,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"63EtYTvnu1r4"},"outputs":[],"source":["%%capture\n","!pip install  tiktoken faiss-cpu\n","!pip install -U sentence-transformers\n","!pip install langchain\n","!pip install pypdf\n","!pip install langchain-huggingface"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4085,"status":"ok","timestamp":1706242394703,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"sjmRd93XvAmj","outputId":"e4e8e34f-508d-44e5-f755-f391688909c5"},"outputs":[],"source":["import os\n","from dotenv import dotenv_values\n","config = dotenv_values(\".env\")\n","#print(config)\n","os.environ[\"GROQ_API_KEY\"] = config[\"GROQ_API_KEY\"]\n"]},{"cell_type":"markdown","metadata":{"id":"NuzRSeWYwOn5"},"source":["# üîç **Retrieval in LangChain Explained**\n","\n","<img src=\"https://python.langchain.com/assets/images/data_connection-95ff2033a8faa5f3ba41376c0f6dd32a.jpg\">\n","<img src=\"https://www.researchgate.net/publication/381125820/figure/fig2/AS:11431281249185289@1717499737731/Illustration-of-a-Retrieval-Augmented-Generation-RAG-workflow-Documents-are-loaded-and.ppm\">\n","\n","### üåê **Basic Concept**\n","\n","Retrieval is like gathering resources to enhance an essay, helping language models access up-to-date, relevant information beyond their built-in knowledge.\n","\n","üí° **Advantages**:\n","   - Adds new, fresh information.\n","   - Makes responses more relevant and informed.\n","\n","üìö **Document Loaders**:\n","   - Function as \"specialized librarians.\"\n","   - Organize content from various sources for language models.\n","\n","üìÑ **Text Loader Fundamentals**:\n","   - Simple process: Converts text files into a usable format for language models.\n"]},{"cell_type":"markdown","metadata":{"id":"onMQEXT7wjMc"},"source":["# üîÑ **Document Loaders in LangChain**:\n","\n","üìã **Wide Selection**: Numerous document loaders available. Check the [documentation](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/document_loaders) for a full list.\n","\n","üë£ **Usage Steps**:\n","   1. Choose a Document Loader from LangChain.\n","   2. Create an instance of the Document Loader.\n","   3. Employ its `load()` method to convert files into LangChain documents.\n","\n","### üõ†Ô∏è **Role of Document Transformers**\n","\n","üìê **Customization for Models**: Adjust documents to suit your model's requirements, like trimming lengthy texts.\n","\n","### ‚úÇÔ∏è **Understanding Text Splitters**\n","\n","üî¢ **Function**: Divide long texts into smaller, coherent segments.\n","\n","üîó **Goal**: Keep related text together, fitting within the model's capacity.\n","\n","### üß© **Using `RecursiveCharacterTextSplitter`**\n","\n","üîÑ **Methodology**:\n","   - Intelligently splits texts using multiple separators.\n","\n","   - Recursively adjusts if segments are too large.\n","\n","   - Ensures all parts are appropriately sized.\n","\n","### üåü **Key Aspects of Splitting**\n","\n","   - Chooses optimal separators for division.\n","\n","   - Continually splits large chunks.\n","\n","   - Balances chunk size by characters or tokens.\n","\n","   - Maintains some overlap for context.\n","\n","   - Tracks chunk starting points if needed.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1706243341736,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"4gItthdswrME"},"outputs":[],"source":["from langchain_community.document_loaders import PyPDFDirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","loader=PyPDFDirectoryLoader(\"./ncs_docs\")\n","docs=loader.load() ## Document Loading  (1GB file)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(type(docs))\n","print(type(docs[0]))\n","len(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap = 200)\n","final_documents=splitter.split_documents(docs[:100]) #splitting\n","print(type(final_documents))\n","print(type(final_documents[0]))\n","print(len(final_documents))\n","print(final_documents[41])\n","print(\"************************\")\n","print(final_documents[42])"]},{"cell_type":"markdown","metadata":{"id":"OsF1E9sUyhL-"},"source":["# üåê **Text Embeddings Overview**\n","\n","üî¢ **Functionality**: Converts documents into numerical vectors in LangChain.\n","\n","ü§ù **Similarity Measure**: Vectors that are closer indicate more similar texts.\n","\n","üîç **Application**: Quickly identify documents with similar topics or content.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","# Define the path to the pre-trained model you want to use\n","modelPath = \"sentence-transformers/all-mpnet-base-v2\"\n","#modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n","\n","# Create a dictionary with model configuration options, specifying to use the CPU for computations\n","model_kwargs = {'device':'cpu'}\n","\n","# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n","encode_kwargs = {'normalize_embeddings': False}\n","\n","# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n","HuggingFaceembeddings = HuggingFaceEmbeddings(\n","    model_name=modelPath,     # Provide the pre-trained model's path\n","    model_kwargs=model_kwargs, # Pass the model configuration options\n","    encode_kwargs=encode_kwargs # Pass the encoding options\n",")"]},{"cell_type":"markdown","metadata":{"id":"Wn6N5TfLq1Js"},"source":["# üõ†Ô∏è **Creating a Vector Store Retriever**\n","\n","1. **Load Documents**: Utilize a document loader for initial document retrieval.\n","\n","2. **Split Texts**: Break down documents into smaller sections with a text splitter.\n","\n","3. **Embedding Conversion**: Apply an embedding model to transform text chunks into vectors.\n","\n","4. **Vector Store Creation**: Compile these vectors into a vector store.\n","\n","üîç **Outcome**: Your vector store is now set up to search and retrieve texts by content."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3941,"status":"ok","timestamp":1706243462944,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"tMTITqujy-Ny"},"outputs":[],"source":["from langchain.vectorstores import FAISS\n","\n","vectorstore = FAISS.from_documents(documents=final_documents, embedding=HuggingFaceembeddings)"]},{"cell_type":"markdown","metadata":{"id":"Ukx_BoFuqWgh"},"source":["# üîé **Vector Store as a Retriever**\n","\n","1. **Search Engine Role**: The vector store functions like a document search engine.\n","\n","2. **Similarity Searches**: Find documents similar to your provided text.\n","\n","3. **Customization Options**: Specify match selectivity and desired number of top results.\n","\n","‚ú® **Functionality**: Use `similarity_search` to pinpoint documents closely matching your specified text, with flexibility in refining search parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":473,"status":"ok","timestamp":1706243566964,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"JoBei3GpskW8","outputId":"c7e40b7a-2b25-45ec-edc5-10d1d628c6fb"},"outputs":[],"source":["query = \"NCS command line\"\n","\n","vectorstore.similarity_search(query)"]},{"cell_type":"markdown","metadata":{"id":"P5quhR5asvju"},"source":["# Generate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":9737,"status":"ok","timestamp":1706243770715,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"},"user_tz":360},"id":"BwZKC0Yzs3ta","outputId":"e370cd9f-dbee-4e61-f9da-ec9b095ad92c"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","\n","from langchain.prompts import PromptTemplate\n","\n","from langchain_groq import ChatGroq\n","\n","template = \"\"\"\n","\n","Use the following pieces of context to answer the question at the end.\n","\n","If you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\n","\n","Use three sentences maximum, relevant analogies, and keep the answer as concise as possible.\n","\n","Use the active voice, and speak directly to the reader using concise language.\n","{context}\n","\n","Question: {question}\n","\n","Helpful Answer:\n","\n","\"\"\"\n","\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","llm = ChatGroq(model=\"llama3-8b-8192\",temperature=0.7)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectorstore.as_retriever(),\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","\n","query = \"can you give an example for NCS command line\"\n","\n","\n","result = qa_chain.invoke({\"query\": query})\n","#print(type(result))\n","result[\"result\"]"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
